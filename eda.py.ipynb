{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Aplicando EDA al dataset Taxi Fare\n",
    "\n",
    "Instalamos las dependencias necesarias para realizar el análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/diego/Desktop/general_venv/lib/python3.9/site-packages (1.2.4)\r\n",
      "Requirement already satisfied: numpy>=1.16.5 in /home/diego/Desktop/general_venv/lib/python3.9/site-packages (from pandas) (1.20.3)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/diego/Desktop/general_venv/lib/python3.9/site-packages (from pandas) (2021.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/diego/Desktop/general_venv/lib/python3.9/site-packages (from pandas) (2.8.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/diego/Desktop/general_venv/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\r\n",
      "Requirement already satisfied: seaborn in /home/diego/Desktop/general_venv/lib/python3.9/site-packages (0.11.1)\r\n",
      "Requirement already satisfied: pandas>=0.23 in /home/diego/Desktop/general_venv/lib/python3.9/site-packages (from seaborn) (1.2.4)\r\n",
      "Requirement already satisfied: scipy>=1.0 in /home/diego/Desktop/general_venv/lib/python3.9/site-packages (from seaborn) (1.6.3)\r\n",
      "Requirement already satisfied: matplotlib>=2.2 in /home/diego/Desktop/general_venv/lib/python3.9/site-packages (from seaborn) (3.4.2)\r\n",
      "Requirement already satisfied: numpy>=1.15 in /home/diego/Desktop/general_venv/lib/python3.9/site-packages (from seaborn) (1.20.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/diego/Desktop/general_venv/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (2.8.1)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/diego/Desktop/general_venv/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (8.2.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/diego/Desktop/general_venv/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/diego/Desktop/general_venv/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /home/diego/Desktop/general_venv/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\r\n",
      "Requirement already satisfied: six in /home/diego/Desktop/general_venv/lib/python3.9/site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.16.0)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/diego/Desktop/general_venv/lib/python3.9/site-packages (from pandas>=0.23->seaborn) (2021.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install seaborn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importamos los módulos a utilizar posteriormente\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'seaborn' from '/home/diego/Desktop/general_venv/lib/python3.9/site-packages/seaborn/__init__.py'>"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "# Esto es solo para la visualización en Pycharm\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "sns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inicializamos algunas constantes a utilizar\n",
    "\n",
    "* **FILE_PATH:** Contiene la ruta de nuestro dataset.\n",
    "* **EARTH_RADIUS:** Es el valor promedio del radio de la Tierra en kilómetros.\n",
    "* **CHUNK_SIZE:** Indica el tamaño del conjunto de datos que se procesará en cada iteración.\n",
    "* **AVAILABLE_CPUS:** Tiene el número de threads que estarán disponibles en el uso de este cuaderno.\n",
    "* **INITIAL_VALID_COLUMNS:** Tiene los nombres de las columnas que nos interesa cargar en memoria."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "('train.csv', 6378.0, 100000, 11)"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILE_PATH = \"train.csv\"\n",
    "EARTH_RADIUS = 6378.0  # Lo utilizamos en el cálculo de la fórmula de Haversine.\n",
    "CHUNK_SIZE = 100000\n",
    "AVAILABLE_CPUS = cpu_count() - 1  # Disminuimos uno del total para evitar que la pc se queda inutilizable.\n",
    "INITIAL_VALID_COLUMNS = ['fare_amount',\n",
    "                         'pickup_datetime',\n",
    "                         'pickup_longitude',\n",
    "                         'pickup_latitude',\n",
    "                         'dropoff_longitude',\n",
    "                         'dropoff_latitude',\n",
    "                         'passenger_count'\n",
    "                         ]\n",
    "\n",
    "FILE_PATH, EARTH_RADIUS, CHUNK_SIZE, AVAILABLE_CPUS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fórmula Haversine\n",
    "\n",
    "Esta fórmula nos servirá para poder calcular la distancia entre 2 puntos geográficos.\n",
    "\n",
    "* TODO Insertar ecuación Haversine en latex\n",
    "\n",
    "Está fórmula será implementada en la función `calculate_haversine_distance`, la cual recibe una columna de tuplas con\n",
    "los puntos de latitud y longitud tanto de la posición en **pickup** como en **drop off**."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "<function __main__.calculate_haversine_distance(pickup_position, drop_off_position)>"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_haversine_distance(pickup_position, drop_off_position):\n",
    "    pickup_lat, pickup_lng = pickup_position\n",
    "    drop_off_lat, drop_off_lng = drop_off_position\n",
    "\n",
    "    pickup_lat, pickup_lng, drop_off_lat, drop_off_lng = map(\n",
    "        radians,\n",
    "        (pickup_lat, pickup_lng, drop_off_lat, drop_off_lng)\n",
    "    )\n",
    "\n",
    "    lat_diff = drop_off_lat - pickup_lat\n",
    "    lng_diff = drop_off_lng - pickup_lng\n",
    "\n",
    "    distance = sin(lat_diff * 0.5) ** 2 + cos(pickup_lat) * cos(drop_off_lat) * sin(lng_diff * 0.5) ** 2\n",
    "\n",
    "    return 2 * EARTH_RADIUS * asin(sqrt(distance))\n",
    "\n",
    "calculate_haversine_distance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Procesamiento de la data\n",
    "\n",
    "El dataset es enorme, tiene **55423857** registros sobre los precios de los taxis, dentro de este dataset tenemos casos\n",
    "donde no existen ciertos registros, o donde tenemos valores bastante extraños, tales como precios negativos o registros\n",
    "donde no existe ningún pasajero, estos registros pueden afectar enormemente los resultados de nuestro modelo, para ello\n",
    "es que debemos procesar la data y eliminar estos valores de nuestro dataset.\n",
    "\n",
    "Al ser un dataset tan grande el tiempo de cómputo necesario para procesar esta data en un solo hilo de nuestro\n",
    "procesador es bastante alto, por lo que debemos paralelizar este proceso, y asegurarnos que se usen al máximo los\n",
    "recursos que tenemos disponibles en nuestra computadora.\n",
    "\n",
    "La librería **Pandas** nos ofrece utilizar un proceso llamado **chunking** que consiste en dividir un gran dataset en\n",
    "pequeños trozos (**chunks**) esto lo logramos pasándole el parámetro `chunksize=(int)` al método `read_csv` de pandas,\n",
    "tal como se observa en la siguiente celda."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "   fare_amount           pickup_datetime  pickup_longitude  pickup_latitude  \\\n0          4.5 2009-06-15 17:26:21+00:00        -73.844311        40.721319   \n1         16.9 2010-01-05 16:52:16+00:00        -74.016048        40.711303   \n2          5.7 2011-08-18 00:35:00+00:00        -73.982738        40.761270   \n3          7.7 2012-04-21 04:30:42+00:00        -73.987130        40.733143   \n4          5.3 2010-03-09 07:51:00+00:00        -73.968095        40.768008   \n\n   dropoff_longitude  dropoff_latitude  passenger_count  \n0         -73.841610         40.712278                1  \n1         -73.979268         40.782004                1  \n2         -73.991242         40.750562                2  \n3         -73.991567         40.758092                1  \n4         -73.956655         40.783762                1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fare_amount</th>\n      <th>pickup_datetime</th>\n      <th>pickup_longitude</th>\n      <th>pickup_latitude</th>\n      <th>dropoff_longitude</th>\n      <th>dropoff_latitude</th>\n      <th>passenger_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.5</td>\n      <td>2009-06-15 17:26:21+00:00</td>\n      <td>-73.844311</td>\n      <td>40.721319</td>\n      <td>-73.841610</td>\n      <td>40.712278</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>16.9</td>\n      <td>2010-01-05 16:52:16+00:00</td>\n      <td>-74.016048</td>\n      <td>40.711303</td>\n      <td>-73.979268</td>\n      <td>40.782004</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5.7</td>\n      <td>2011-08-18 00:35:00+00:00</td>\n      <td>-73.982738</td>\n      <td>40.761270</td>\n      <td>-73.991242</td>\n      <td>40.750562</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7.7</td>\n      <td>2012-04-21 04:30:42+00:00</td>\n      <td>-73.987130</td>\n      <td>40.733143</td>\n      <td>-73.991567</td>\n      <td>40.758092</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.3</td>\n      <td>2010-03-09 07:51:00+00:00</td>\n      <td>-73.968095</td>\n      <td>40.768008</td>\n      <td>-73.956655</td>\n      <td>40.783762</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks = pd.read_csv(FILE_PATH, chunksize=CHUNK_SIZE, parse_dates=[\"pickup_datetime\"], usecols=INITIAL_VALID_COLUMNS)\n",
    "\n",
    "# Iteramos sobre df_chunks y obtenemos una primera vista del dataset\n",
    "df_chunks.__next__().head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En la celda anterior podemos observar que además de pasarle el parámetro `chunksize` al método `read_csv` también\n",
    "le pasamos un parámetro `pase_dates` a la columna `pickup_datetime`, esto es debido a que previamente se revisó el\n",
    "dataset y observamos que este campo tenía datos del tipo `2009-06-15 17:26:21 UTC` el cual puede ser parseado por pandas\n",
    "a `2009-06-15 17:26:21+00:00` que es un dato del tipo `datetime` y que nos servirá posteriormente para extraer cada una\n",
    "de sus partes tales como el **año**, **mes**, **día** y **hora**.\n",
    "\n",
    "Otro parámetro pasado al método fue `usecols` el cual nos sirve para indicar que columnas queremos cargar en memoria.\n",
    "La columna `key` no es necesaria debido a que es una copia de la columna `pickup_datetime`, podríamos eliminarla\n",
    "posteriormente pero en temas de memoria esa columna extra al ser del tipo `object` nos quita innecesariamente espacio."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Función de paralelización\n",
    "\n",
    "Como se mencionó anteriormente llevaremos el procesamiento del dataset de forma paralela para ello utilizaremos el\n",
    "módulo multiprocessing de python, específicamente la clase Pool que nos permitirá asignar un proceso a cada uno de\n",
    "los hilos disponibles."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "([], <function __main__.parallelize_chunk_processing(chunk, func)>)"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable que guardará todos los resultados procesados en cada iteración.\n",
    "chunk_list = []\n",
    "\n",
    "def parallelize_chunk_processing(chunk, func):\n",
    "\n",
    "    # Dividimos cada chunk en partes más pequeñas que son las que serán procesadas por cada hilo del procesador\n",
    "    chunk_split = np.array_split(chunk, AVAILABLE_CPUS)\n",
    "\n",
    "    # Creamos un pool de n hilos donde n es el número asignado previamente a **AVAILABLE_CPUS**\n",
    "    pool = Pool(AVAILABLE_CPUS)\n",
    "\n",
    "    # Creamos un nuevo dataset a partir de los resultados procesados en cada hilo.\n",
    "    chunk = pd.concat(pool.map(func, chunk_split))\n",
    "\n",
    "    # Cerramos el pool y creamos una barrera con el método join\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    chunk_list.append(chunk)\n",
    "\n",
    "chunk_list, parallelize_chunk_processing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Una vez definida la función de paralelización procedemos con el tratamiento del dataset, tras varias pruebas decidimos\n",
    "que se deben aplicar las siguientes modificaciones al dataset original.\n",
    "\n",
    "* Eliminar los `nan` detectados por pandas al cargar el `chunk`\n",
    "* Reemplazar los valores `zero` en las columnas que no deberían tener dicho valor con un valor nan, tales como\n",
    "**pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, passenger_count**.\n",
    "* Obtener los campos **año, mes, dia, hora** del campo `pickup_datetime`.\n",
    "* Obtener la distancia entre los puntos de **pickup** y **drop off**.\n",
    "* Eliminar las columnas que ya no serán útiles y eliminar los valores `nan` creados por nosotros.\n",
    "* Eliminar los precios negativos (**Campo `fare_amount`**)\n",
    "* **TODO** Agregar más cosas a procesar, ya sea al final o entre los otros pasos.\n",
    "\n",
    "Para todo esto definimos la función `process_chunk` que recibirá como parámetro el chunk de cada iteración."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "def process_chunk(split_chunk: pd.DataFrame):\n",
    "\n",
    "    # Eliminamos los valores nan encontrados por pandas\n",
    "    split_chunk.dropna(inplace=True)\n",
    "\n",
    "    # Volvemos **nan** aquellas columnas que no deben tener valores zero\n",
    "    to_nan_columns = [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"passenger_count\"]\n",
    "    split_chunk[to_nan_columns] = split_chunk[to_nan_columns].replace(0, np.nan)\n",
    "\n",
    "    # Extraemos los campos año, mes, dia y hora del campo **pickup_datetime**\n",
    "    split_chunk[\"year\"] = split_chunk[\"pickup_datetime\"].dt.year\n",
    "    split_chunk[\"month\"] = split_chunk[\"pickup_datetime\"].dt.month\n",
    "    split_chunk[\"day\"] = split_chunk[\"pickup_datetime\"].dt.day\n",
    "    split_chunk[\"hour\"] = split_chunk[\"pickup_datetime\"].dt.hour\n",
    "\n",
    "    # Calculamos la distancia utilizando la fórmula Haversine\n",
    "    split_chunk['distance'] = split_chunk.apply(lambda row: calculate_haversine_distance(\n",
    "        pickup_position=(row[\"pickup_latitude\"], row[\"pickup_longitude\"]),\n",
    "        drop_off_position=(row[\"dropoff_latitude\"], row[\"dropoff_longitude\"])), axis=1)\n",
    "\n",
    "    # Volvemos **nan** aquellas distancias con valor cero\n",
    "    split_chunk[\"distance\"] = split_chunk[\"distance\"].replace(0, np.nan)\n",
    "\n",
    "    split_chunk.dropna(inplace=True)\n",
    "    split_chunk.drop(to_nan_columns, axis=1, inplace=True)\n",
    "    split_chunk.drop(\"pickup_datetime\", axis=1, inplace=True)\n",
    "\n",
    "    test = split_chunk.drop(split_chunk[split_chunk.fare_amount <= 0].index)\n",
    "\n",
    "    # Testing cause of not parallel process\n",
    "    return test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ya tenemos definida la función que nos servirá para procesar el dataset, ahora deberemos iterar sobre el objeto\n",
    "`df_chunks` que inicializamos anteriormente."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "for df_chunk in df_chunks:\n",
    "    parallelize_chunk_processing(df_chunk, process_chunk)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cuando el proceso termine tendremos los resultados procesados en la variable `chunk_list` que es lo que utilizaremos\n",
    "para crear un nuevo dataframe."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "        fare_amount  year  month  day  hour   distance\n100000          6.9  2011      3   10     9   1.949855\n100001          6.5  2014      3   27     7   2.119317\n100002         33.0  2014      9   26    11  10.197408\n100003          6.5  2015      4   28     7   1.092809\n100004         11.3  2010      4   21    22   4.162837",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fare_amount</th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>hour</th>\n      <th>distance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>100000</th>\n      <td>6.9</td>\n      <td>2011</td>\n      <td>3</td>\n      <td>10</td>\n      <td>9</td>\n      <td>1.949855</td>\n    </tr>\n    <tr>\n      <th>100001</th>\n      <td>6.5</td>\n      <td>2014</td>\n      <td>3</td>\n      <td>27</td>\n      <td>7</td>\n      <td>2.119317</td>\n    </tr>\n    <tr>\n      <th>100002</th>\n      <td>33.0</td>\n      <td>2014</td>\n      <td>9</td>\n      <td>26</td>\n      <td>11</td>\n      <td>10.197408</td>\n    </tr>\n    <tr>\n      <th>100003</th>\n      <td>6.5</td>\n      <td>2015</td>\n      <td>4</td>\n      <td>28</td>\n      <td>7</td>\n      <td>1.092809</td>\n    </tr>\n    <tr>\n      <th>100004</th>\n      <td>11.3</td>\n      <td>2010</td>\n      <td>4</td>\n      <td>21</td>\n      <td>22</td>\n      <td>4.162837</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat(chunk_list)\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}